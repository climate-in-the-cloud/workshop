{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimal-interpolation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/climate-in-the-cloud/workshop/blob/master/optimal-interpolation/optimal_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WRTN77yA_Hq"
      },
      "source": [
        "![](https://raw.githubusercontent.com/climate-in-the-cloud/workshop/master/optimal-interpolation/mercator_temperature.png)\n",
        "\n",
        "# Lab C: Optimal Interpolation\n",
        "\n",
        "In this lab, we will look at some ocean model data and use it to calculate a covariance function fitted to some \"measurements\". Using this covariance function, we can use a process called optimal interpolation to estimate the state of the ocean using our available measurements. \n",
        "\n",
        "By the end of this lab, you will be able to \n",
        "\n",
        "- import data into the Colab environment\n",
        "- create synthetic \"measurements\" of ocean temperature data by subsampling the dataset and adding random errors\n",
        "- compute the lagged binned covariance function from the data and fit it to standard functional forms \n",
        "- calculate the data covariance matrix using the fitted covariance function\n",
        "- Reconstruct an estimate of the temperature at every point using optimal interpolation. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-bhMHw1GLKV"
      },
      "source": [
        "## Background\n",
        "\n",
        "The covariance is an important quantity in optimal interpolation and other inverse problems. It gives us an estimate of the uncertainty in our model or data, which tells us how much we should trust this when we use it in our state estimate. \n",
        "\n",
        "The covariance is often calculated empirically, that is, from data. We want to use a large enough data set so that our estimate will be *unbiased*, that is, the mean of the data is close to the mean of the true state of the system. If we use too small a data set, our estimate can become biased and our estimate will be a poor representation of the true state. \n",
        "\n",
        "In this lab, we will make a large number of synthetic measurements of ocean temperature and use these to construct an empirical covariance function. We will then use this function to evaluate the covariance matrix of the data, and use this to create an optimally interpolated estimate of the data everywhere. \n",
        "\n",
        "The data set we will use is ocean temperature data in the North Atlantic from [Mercator Ocean](http://bulletin.mercator-ocean.fr), the French operational ocean forecast system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUNR4Cw6Hs3V"
      },
      "source": [
        "# An introduction to Python\n",
        "\n",
        "In this course, we will use [Python](https://www.python.org/) to run fluid dynamics simulations, analyze output, and plot figures and movies. Python is free, flexible, easy to use, and has tons of online resources for beginners. If you have a question about Python, my default answer will be \"have you Googled it?\"\n",
        "\n",
        "You do not need to be fluent in Python or any other programming language for this course. We will run Python from within these notebooks, and you will get lots of guidance. You are also encouraged to complete this [free online tutorial](https://www.codecademy.com/learn/learn-python) to familiarise yourself with Python syntax.\n",
        "\n",
        "If you are comfortable with Python and the Colab notebook environment, you can skip this section and move straight to \"Experimental setup\". \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDlp3ZtZ_os8"
      },
      "source": [
        "## Google Colaboratory\n",
        "\n",
        "[Google Colaboratory](http://colab.research.google.com/) (or \"Colab\", for short) is a python notebook that you can run in a browser window. Colab notebooks are hosted on the cloud, which means that you do not need to have python installed on your local machine --- you can run it straight from a web browser. Watch [Introduction to Colab](https://www.youtube.com/watch?v=inN8seMm7UI)  to learn more, or check out this [Overview of Colab Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb). \n",
        "\n",
        "You can sign up for a (free) [Google Account](https://myaccount.google.com/intro) to run the notebooks. This will allow you to save your notes, output, and so on and also connect to Google Drive. If you would prefer not to sign up, you can still run the notebook by selecting File > Open in Playground Mode in the menu bar. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fIQNgTCAu5"
      },
      "source": [
        "## Python notebooks\n",
        "\n",
        "We will run python scripts in interective, web-based, computational environments called notebooks. Within a notebook you can display text, mathematical notation, images, etc, using [Markdown](https://www.markdownguide.org/). For example:\n",
        "\n",
        "### World's most awesome equations: \n",
        "\n",
        "- Newton's second law: $F = m \\dot{v}$. \n",
        "- Euler's equation: $e^{\\mathrm{i} \\pi} + 1 = 0$. \n",
        "- Wave equation (\"_gnarly dude!_\"): \n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial t^2} = c^2 \\frac{\\partial^2 f}{\\partial x^2}.\n",
        "$$\n",
        "\n",
        "\n",
        "If you double-click any cell, you can see the Markdown code used to create it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfU6nvoBXJs"
      },
      "source": [
        "Notebooks also allow you to use \"cells\" of python code that be executed in real time. \n",
        "\n",
        "To see this in action, move down to the next cell and enter the following python code: \n",
        "\n",
        "```\n",
        "string = \"Hello world!\"\n",
        "print(string)\n",
        "```\n",
        "\n",
        "When you are finished, press **SHIFT + ENTER** to run the cell (or press the **Play button** on the left-side of the code cell). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuHQKhltCJWR"
      },
      "source": [
        "# This is a comment reminding you to enter your python code below. Don't forget to press shift + enter to run!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9StXqW8sCi8p"
      },
      "source": [
        "Congratulations! You are now a pythonista. Variables carry over from cell to cell, so you can build complex scripts by sequentially running cells one after another. \n",
        "\n",
        "Now create a new cell by clicking on the \"+\" button in the menu above. Make sure that the cell type is \"Code\" in the drop down menu. Navigate to the new cell and enter the following python code: \n",
        "\n",
        "```\n",
        "longer_string = string + \" spam! Spam! SPAM!\"\n",
        "print(longer_string)\n",
        "```\n",
        "\n",
        "Don't forget to **SHIFT + ENTER** to run the cell. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slMkDPnxCnOJ"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "Python makes extensive use of freely available, open source *libraries*, which contain tonnes of useful functions etc that you can make use of. The following python code calls two standard libraries: `numpy` (pronounced \"numb pie\"), which contains useful functions for carrying out numerics, and `matplotlib`, which allows us to plot data.\n",
        "\n",
        "Since we will be using these libraries repeatedly, we will abbreviate their names to `np` and `plt`, respectively.\n",
        "\n",
        "Finally, the last line instructs `matplotlib` to plot figures in the notebook, just below the cell that calls it. That way, you will be able to view and save figures within the notebook. \n",
        "\n",
        "**Move to the next cell by pressing DOWN, then press SHIFT + ENTER to run (or click \"Run\" in the menu above)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_egAKoCCtFu"
      },
      "source": [
        "# Numerics\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot figures in Jupyter notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZgHOllSCsQ1"
      },
      "source": [
        "## Variables, arrays, and indexing\n",
        "\n",
        "The first thing we need to do is define a numerical grid. We will do this using the `numpy` functions `linspace` and `meshgrid`. Thus, when we call them, we will use the alias we defined for this library, `np`, followed by a period `.` followed by the particular function we want from the library, e.g. `np.linspace`.\n",
        "\n",
        "First, we create a vector `xv` of `2n+1` equally spaced gridpoints between $x = -1$ and $x = 1$. Again, press **shift + enter** to run the following cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gpsOkD5C06u"
      },
      "source": [
        "# Grid\n",
        "n = 50\n",
        "xv = np.linspace(-1,1,2*n+1)\n",
        "\n",
        "print(xv.shape)\n",
        "print(xv[0:10])\n",
        "print(xv[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFIk1g3SCzn-"
      },
      "source": [
        "The command `print(xv.shape)` prints the dimensions of the array `xv`. In this case, it is a 1D array with `2n+1` elements, so the dimensions are $(2n+1,)$. Notice that the second dimension is simply blank, rather than `1`. \n",
        "\n",
        "The commands `print(xv[0:10])` and `print(xv[-10:])` print the first and last ten elements of the array. Indexing in python is indicated using square brackets `[...]` and is referenced to the first element (for positive indices) or the last element (for negative indices). Thus, `xv[0:10]` can be read as: \"first + 0 elements to first + 10 elements\", and `xv[-10:]` can be read as: \"last - 10 elements to last element\". \n",
        "\n",
        "Next we will define 2D arrays `xx` and `yy` using the function `meshgrid`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkUVt5ODLZD"
      },
      "source": [
        "xx,yy = np.meshgrid(xv,xv,indexing='ij')\n",
        "\n",
        "print(xx.shape)\n",
        "print(yy.shape)\n",
        "\n",
        "print(xx[0:10,0:10])\n",
        "print(yy[0:10,0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOhJfXErDVxX"
      },
      "source": [
        "The array `xx` is an $2n+1 \\times 2n+1$ matrix in which each row is the corresponding value of `xv`. Likewise, the array `yy` is an $2n+1 \\times 2n+1$ matrix with each column having the corresponding value of `xv`. \n",
        "\n",
        "Let's try plotting a simple function of x and y using the arrays `xx` and `yy`. The function we will use is \n",
        "\n",
        "$$\n",
        "f = \\sin  (\\pi x) \n",
        "$$\n",
        "\n",
        "Both the sine function and the constant pi are part of the `numpy` library, so we call the using `np.sin` and `np.pi`, respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSSw5ohpDZQ3"
      },
      "source": [
        "# define function\n",
        "f = np.sin(np.pi*xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRKVWJoKDcy6"
      },
      "source": [
        "We will make a 2D colorplot of this function using `plt.pcolormesh`, as well as a 1D plot of a slice of the function using `plt.plot`. We will also use some other functions from `matplotlib` to control the appearance of the figure, such as `subplot`, `title`, `xlabel`, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfQq_MxwDfFb"
      },
      "source": [
        "# plot 2D colorplot\n",
        "plt.subplot(1,2,1)\n",
        "plt.pcolormesh(xx, yy, f)\n",
        "plt.title('f(x,y)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('square')\n",
        "\n",
        "# plot 1D slice along y = 0\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(xv,f[:,n])\n",
        "plt.title('f(x,0)')\n",
        "plt.xlabel('x')\n",
        "plt.axis('square')\n",
        "plt.ylim(-1.1,1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty_CRiDDgDj"
      },
      "source": [
        "--------------\n",
        "\n",
        "### Exercises\n",
        "\n",
        "- Modify the function `f` above so that it is $f = \\cos (\\pi y)$ and plot a 2D colormap of the function. To do this, you will need to run (**shift + enter**) both cells: the cell that defines `f` and the cell that plots `f`. \n",
        "\n",
        "- How should you change the 1D plot so that it shows a slice along $x = 0$ instead of $y = 0$? \n",
        "\n",
        "- Try plotting a more complication function like $f = \\sin (\\pi x) \\cos (\\pi y$). \n",
        "\n",
        "------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPZLxMLv_dcs"
      },
      "source": [
        "# Experimental setup\n",
        "\n",
        "\n",
        "### Libraries\n",
        "\n",
        "Let's begin by importing some useful libraries. You will see two new libraries here, `h5py`, which is used to import data that has been save in the HDF5 format, and `random` which we will use to create random errors and samples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX0qdVsQJ0nj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAcYPfFZIFKl"
      },
      "source": [
        "### Upload the data\n",
        "\n",
        "The data has been saved in a file called `mercator_temp.h5` and can be found on the [GitHub repository](https://github.com/climate-in-the-cloud/workshop/tree/master/optimal-interpolation) for this lab: \n",
        "\n",
        "```\n",
        "https://github.com/climate-in-the-cloud/workshop/tree/master/optimal-interpolation\n",
        "```\n",
        "\n",
        "You should download this file to your local machine (it is about 3 Mb). \n",
        "\n",
        "Remember, the local directory of the Colab session is not the same as your own local directory, so you will need to upload the file in order to be able to manipulate it in Colab. There are a number of ways to do this: \n",
        "\n",
        "- The most straightforward method is to use the upload functionality of Colab. Navigate to the menu bar on the left of the Colab window and click on the folder icon. You may need to wait a moment to let Colab initialize the runtime. You will be able to see a file tree (these are the files available to Colab). You will also see an \"Upload\" button which you can use to upload the file to Colab. \n",
        "\n",
        "- If you have a Google account, you can mount your Google drive within the Colab environment: \n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls drive/'My Drive'\n",
        "```\n",
        "You will need to add the file to your Google Drive. From there it will now be visible to Colab. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcRl91b5KFU3"
      },
      "source": [
        "### Import the data into the Colab environment\n",
        "\n",
        "Once the data file `mercator_temp.h5` has been uploaded to Colab, it needs to be imported into the Colab notebook environment. We will do this using some tools from the `h5py` library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGlKsY0TCHtL"
      },
      "source": [
        "# load temperature and grid data\n",
        "\n",
        "file = h5py.File('mercator_temp.h5','r')\n",
        "temp = file['/data/temp'][:]\n",
        "lon  = file['/data/x'][:]\n",
        "lat  = file['/data/y'][:]\n",
        "z    = file['/data/z'][:]\n",
        "\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wtTK1RDKZh6"
      },
      "source": [
        "### Plot the data\n",
        "\n",
        "Let's start off by plotting the data to see what it looks like. The temperature field `temp` is a 3D data cube (latitude x longitude x depth). To plot a 2D snapshot of the temperature, we can specify a particular depth index. Once we have done that, we can plot the data using a `meshgrid` of the latitude and longitude coordinates (`x` and `y`). \n",
        "\n",
        "This type of plot is called (appropriately enough) a *Mercator projection*. In this projection, a degree of latitude or longitude has the same length everywhere on the globe. This leads to some distortions, especially at higher latitudes. For example, on a typical world map using a Mercator projection, Greenland and Africa look approximately the same size, when in fact [Africa is 14 times larger than Greenland](https://www.youtube.com/watch?v=vVX-PrBRtTY). In this lab, we be looking at a limited degree box of the North Atlantic, so this is not such a big problem, although we will need to convert to a local cartesian coordinate system to calculate distances. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAoxi2fkKpYp"
      },
      "source": [
        "k = 10        # depth level for temperature data\n",
        "\n",
        "lon_,lat_ = np.meshgrid(lon,lat,indexing='ij') # create a meshgrid of lat/lon\n",
        "\n",
        "# plot the data\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.contourf(lon_, lat_, temp[:,:,k],32,cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Temperature at depth ' + str(z[0,k]) + 'm')\n",
        "cbar = fig.colorbar(cax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPaJ6PVN-zI"
      },
      "source": [
        "-------------------\n",
        "**Exercise**\n",
        "\n",
        "- Can you plot an east-west (zonal) slice of the data on a longitude-depth axis? \n",
        "\n",
        "- How about a north-south (meridional) slice on a latitude-depth axis? \n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSfAkrgmOcVh"
      },
      "source": [
        "### Create synthetic observations\n",
        "\n",
        "We will now subsample the data at random grid points to simulate a series of observations over the domain. For example, these observations could come drifting profiling floats (such as the [Argo network](http://www.argo.ucsd.edu/About_Argo.html)). We will also add some random errors to simulate measurement noise.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vrvKTI5hMJo"
      },
      "source": [
        "frac = 0.1    # fraction of data to retain \n",
        "e = 0.4       # random error to add to obs (deg C)\n",
        "k = 10        # depth level for temperature data\n",
        "\n",
        "# flatten the coordinate and temperature arrays in 1D arrays\n",
        "lonv = lon_.flatten()\n",
        "latv = lat_.flatten()\n",
        "data = temp[:,:,k].flatten()\n",
        "\n",
        "# remove land data (indicated by NaNs in our dataset)\n",
        "lonv = lonv[np.logical_not(np.isnan(data))]\n",
        "latv = latv[np.logical_not(np.isnan(data))]\n",
        "data = data[np.logical_not(np.isnan(data))]\n",
        "\n",
        "# randomly permute the grid points so the first Nm are observed \n",
        "r = np.random.permutation(range(0,data.size))\n",
        "Nm = round(frac*data.size) # number of observed data points\n",
        "Ni = data.size - Nm        # number of unobserved (interpolated) data points\n",
        "rm = r[0:Nm]\n",
        "ri = r[Nm:]\n",
        "\n",
        "# subsample the data and add random measurement errors\n",
        "lonm = lonv[rm]\n",
        "latm = latv[rm]\n",
        "vm = data[rm] + e*np.random.normal(0,1,rm.size)\n",
        "\n",
        "# unsampled data (missing gridpoints)\n",
        "loni = lonv[ri]\n",
        "lati = latv[ri]\n",
        "vi = data[ri]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBiiET1PdZe"
      },
      "source": [
        "### Plot the measurements\n",
        "\n",
        "Now let's compare the try ocean state with our observations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y8nl0UOClJT"
      },
      "source": [
        "lon_,lat_ = np.meshgrid(lon,lat,indexing='ij')\n",
        "\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "\n",
        "ax  = fig.add_subplot(1,2,1)\n",
        "cax = ax.contourf(lon_, lat_, temp[:,:,k],32,cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Temperature at depth ' + str(z[0,k]) + 'm')\n",
        "cbar = fig.colorbar(cax)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "cax = ax.scatter(lonm, latm, c=vm, s=50, edgecolors=\"none\",cmap=plt.cm.coolwarm, vmax=24,vmin=2.4)\n",
        "ax.set(title='Subsampled ' + str(round(100*Nm/data.size,2)) + '% of data')\n",
        "cbar = fig.colorbar(cax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqU4iLvZRkR6"
      },
      "source": [
        "## Calculate the covariance between data points\n",
        "\n",
        "We will now make an estimate of the data covariance function by computing binned-lagged covariance for all data pairs. We first need to know how the distance between every pair of data points. We will do this by converting latitude and longitude to kilometers using a cartesian plane approximation to the sphere:\n",
        "\n",
        "$$\n",
        "x \\approx R_e ( \\theta - \\theta_0 ) \\cos \\left( \\lambda \\right) \\\\\n",
        "y \\approx R_e \\left( \\lambda - \\lambda_0 \\right)  \n",
        "$$\n",
        "\n",
        "where $R_e$ is Earth's radius, ($\\theta$, $\\lambda$) are the longitude and latitude (in radians), and ($\\theta_0$, $\\lambda_0$) are the latitude and longitude where the cartesian plane touches the sphere (i.e. the middle of the plane). \n",
        "\n",
        "Once we have approximate cartesian coordinates for the data points, we will create a `meshgrid` $X$ and $Y$ and calculate the distance between points using\n",
        "\n",
        "$$ \n",
        "dX = X - X^T, \\quad dY = Y - Y^Y.\n",
        "$$\n",
        "\n",
        "Here, $X$ and $Y$ are $Nm \\times Nm$ matrices ($Nm$ is the number of data points) and $X^T$ is the transpose of $X$. \n",
        "\n",
        "The cartesian distance between data points $m$ and $n$ is then \n",
        "\n",
        "$$\n",
        "R_{mn} = \\left( dX_{mn}^2 + dY_{mn}^2 \\right) \n",
        "$$\n",
        "\n",
        "The resulting matrix $\\mathsf{R}_{dd}$ is symmetric (because the distance between $m$ and $n$ is the same as the distance between point $n$ and $m$) with a row of zeros along the diagonal (because the distance between $m$ and itself is zero). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwHRe87-S0YF"
      },
      "source": [
        "# Get distances separating every pair of stations using approximate\n",
        "# spherical geometry\n",
        "lon0 = (np.max(lon)+np.min(lon))/2\n",
        "lat0 = (np.max(lat)+np.min(lat))/2\n",
        "rearth = 6370.8; # kilometres\n",
        "\n",
        "# xm, ym will be the coordinates of the data (converted from lon/lat)\n",
        "xm = []\n",
        "ym = []\n",
        "\n",
        "for m in range(0,Nm):\n",
        "  xm.append(rearth*(np.pi/180*(lonm[m]-lon0))*np.cos(np.pi/180*latm[m]))\n",
        "  ym.append(rearth*(latm[m]-lat0)*np.pi/180)\n",
        "\n",
        "xm_,ym_ = np.meshgrid(xm,ym,indexing='ij')\n",
        "\n",
        "dxm = xm_ - np.transpose(xm_)\n",
        "dym = ym_ - np.transpose(ym_)\n",
        "\n",
        "# Note that in python * indicates elementwise multiplication, not matrix multiplication. \n",
        "Rdd = np.sqrt(dxm*dxm + dym*dym)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O5Vbom-VFRw"
      },
      "source": [
        "### Binned lagged covariance function\n",
        "\n",
        "The covariance is calculated from the data $v_m$ as \n",
        "\n",
        "$$\n",
        "C_{mn} = \\left( v_m - \\overline{v} \\right) \\left( v_n - \\overline{v} \\right)\n",
        "$$\n",
        "\n",
        "where $\\overline{v}$ is the average value of $v_m$. \n",
        "\n",
        "Assuming isotropy (direction independence) and homogeniety (translation independence), the covariance will be a function of distance only, ie $C = C(r)$. We will now compute the covariance as a function of distance by binning by distance. \n",
        "\n",
        "Note that the assumptions of homogeneity and isotropy generally do not hold in the ocean, but they are acceptable across a limited range of scales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UWUpPOFvEYz"
      },
      "source": [
        "## Compute the binnned lagged covariance function\n",
        "\n",
        "# remove mean before calculating covariance\n",
        "d = vm-np.mean(vm)\n",
        "\n",
        "# covariance\n",
        "C = np.tensordot(d,d,axes=0)\n",
        "\n",
        "# set the distance bins\n",
        "dr = 30\n",
        "nbins = int(np.floor(1000/dr)) + 1\n",
        "r = np.zeros(nbins)\n",
        "r[1:] = np.linspace(15,975,nbins-1)\n",
        "\n",
        "# variance of samples that fall into each bin\n",
        "cf = np.zeros(nbins)\n",
        "cf[0] = np.mean(np.diag(C))\n",
        "\n",
        "for b in range(1,nbins):\n",
        "  ind = np.where( (Rdd > r[b]-0.5*dr) & (Rdd <= r[b]+0.5*dr))\n",
        "  if ind[0].size > 0:\n",
        "    cf[b] = np.mean(C[ind[0],ind[1]])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.scatter(r, cf, s=50, edgecolors=\"none\")\n",
        "ax.set(title='Binned lagged covariance from data')\n",
        "ax.set(xlabel='Distance (km)')\n",
        "ax.set(ylabel='deg C^2')\n",
        "cbar = fig.colorbar(cax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7j_IbDqQ61k"
      },
      "source": [
        "## Fit the covariance\n",
        "\n",
        "We would like to be able to capture the essential features of the empirical covariance $C(r)$ in just a few parameters. We next fit the empirical covariance to a simple functional form, \n",
        "\n",
        "$$\n",
        "C_G (r) = a \\exp \\left( - \\frac{r^2}{b^2} \\right) \n",
        "$$\n",
        "\n",
        "This is a Gaussian with amplitude $a$ and width $b$. If we can estimate the values of $a$ and $b$ from the data, we will be able to use this functional form to estimate the covariance of any two points in space. \n",
        "\n",
        "There are many options for fitting functions like this. We will use a very simple approach. The value of $C_G(0)$ is given by \n",
        "\n",
        "$$\n",
        "C_G(0) = a.\n",
        "$$\n",
        "\n",
        "Thus, we can determine $a$ by demanding that the Gaussian fit matches the empirical covariance as $r \\rightarrow 0$. (Note that we don't want to fit it exactly at zero because the covariance has some measurement noise at $r = 0$). \n",
        "\n",
        "Next we look at the total area under the curve of $C_G(r)$. The integral of Gaussian is non elementary, but we can calculate the integral $\\int_0^\\infty C_G(r) dr$ using the following identity. \n",
        "\n",
        "$$\n",
        "\\int_{-\\infty}^\\infty a \\exp\\left( - \\frac{x^2}{b^2} \\right) dx = \\sqrt{\\pi} a \\lvert b \\rvert. \n",
        "$$\n",
        "\n",
        "We demand that the total area under the curve of $C_G(r)$ matches the total area under the curve of $C (r)$.However, notice that, for values of $r > 600$ km, the covariance $C(r)$ is negative. Therefore, we will only integrate $C(r)$ up to $r = 600$ km. From this we can estimate $b$. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XZ216Viy_v9"
      },
      "source": [
        "# fit the covariance to a Gaussian\n",
        "\n",
        "ind = np.where(r > 600)\n",
        "ind = ind[0][0]-1\n",
        "\n",
        "c0 = cf[2] # don't fit to r = 0\n",
        "cint = np.sum(cf[1:ind])*dr # integral up to where C(r) crosses the r-axis\n",
        "a = c0\n",
        "b = 2*cint/(a*np.sqrt(np.pi))\n",
        "\n",
        "# functional fit\n",
        "cgauss = a*np.exp(-r**2/b**2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.scatter(r, cf, s=50, edgecolors=\"none\")\n",
        "ax.set(title='Binned lagged covariance from data')\n",
        "ax.set(xlabel='Distance (km)')\n",
        "ax.set(ylabel='deg C^2')\n",
        "cbar = fig.colorbar(cax)\n",
        "gax = ax.plot(r,cgauss)\n",
        "\n",
        "# calculate misfit between covariance and fitted functions\n",
        "misfit_gauss  = np.mean((cf[1:ind]-cgauss[1:ind])**2)\n",
        "esq_gauss  = np.abs(cf[0] - cgauss[0])\n",
        "\n",
        "print('Gauss misfit   = ' + str(round(misfit_gauss,4)))\n",
        "print('std error      = ' + str(round(np.sqrt(esq_gauss),4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWPwBtL_QJhh"
      },
      "source": [
        "Why is the covariance of the observed points negative past 600km?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUaQlVrwYzIu"
      },
      "source": [
        "## Rebuild the data-data covariance matrix\n",
        "\n",
        "Now that we have an approximate functional fit to the covariance between any two data points, we can use this to recreate the data-data covariance matrix. Notice that the new data-data covariance matrix uses only four pieces of information, the distance between the points, the parameters $a$ and $b$, and the functional form (Gaussian). This is a dramatic reduction in complexity compared to the full covariance matrix, which was calculated directly from the data itself. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CYy5XNrTS19"
      },
      "source": [
        "# Build the data-data covariance (normalized to be 1 at r = 0)\n",
        "Cdd0 = np.exp(-Rdd**2/b**2)\n",
        "\n",
        "# Add error/signal variance to the diagonal\n",
        "lam = esq_gauss/cgauss[0]\n",
        "Cdd = Cdd0 + np.diag(lam*np.ones(Nm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMv-oHYPaGmQ"
      },
      "source": [
        "### Interpolate to the data points\n",
        "\n",
        "Now let's see how good a job our fitted covariance function does. Let's use the fitted covariance function to interpolate our data back to the original data points. We can then compare these with the measured values at those locations to measure the performance of the fitted covariance function. \n",
        "\n",
        "The general analysis equation is\n",
        "$$\n",
        "D = \\overline{v} + W \\left( v_m - \\overline{v} \\right),\n",
        "$$\n",
        "where $D$ is the analysis estimate and $W$ is the weight matrix that for simplified optimal interpolation can be described as\n",
        "$$\n",
        "W = C_{im}^{model}\\centerdot C_{mm}^{-1}.\n",
        "$$\n",
        "Here $C$ represents the covariance between the interpolated points, denoted by the subscript $i$, and the point observations, denoted by $m$.\n",
        "\n",
        "Note that this is actual a simplified form of optimal interpolation. In this case our prior is the mean temperature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLiXCO4haHO5"
      },
      "source": [
        "# interpolated data\n",
        "D0 = np.mean(vm) + np.matmul(Cdd0,np.matmul(np.linalg.inv(Cdd),(np.array(vm)-np.mean(vm))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2hCYww_a6Z9"
      },
      "source": [
        "fig = plt.figure(figsize=(16,6))\n",
        "\n",
        "ax  = fig.add_subplot(1,2,1)\n",
        "cax = ax.contourf(lon_, lat_, temp[:,:,k],32,cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Temperature at depth ' + str(z[0,k]) + 'm')\n",
        "cbar = fig.colorbar(cax)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "cax = ax.scatter(lonm, latm, c=D0, s=50, edgecolors=\"none\",cmap=plt.cm.coolwarm,vmax=24, vmin=2.4)\n",
        "ax.set(title='Reconstructed data (deg C)')\n",
        "cbar = fig.colorbar(cax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-M0u-mmI9DS"
      },
      "source": [
        "---\n",
        "### Exercises\n",
        "\n",
        "- Instead of plotting the reconstructed data, try plotting the difference between the true temperature and the reconstructed temperature. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE19zWIEZ2Ag"
      },
      "source": [
        "# Get distances separating every missing data point and every observation using\n",
        "# approximate spherical geometry\n",
        "\n",
        "lon0 = (np.max(lon)+np.min(lon))/2\n",
        "lat0 = (np.max(lat)+np.min(lat))/2\n",
        "rearth = 6370.8; # kilometres\n",
        "\n",
        "# xi, yi will be the coordinates of the interpolated data (converted from lon/lat)\n",
        "xi = []\n",
        "yi = []\n",
        "\n",
        "for m in range(0,Ni):\n",
        "  xi.append(rearth*(np.pi/180*(loni[m]-lon0))*np.cos(np.pi/180*lati[m]))\n",
        "  yi.append(rearth*(lati[m]-lat0)*np.pi/180)\n",
        "\n",
        "\n",
        "xi_,xm_ = np.meshgrid(xi,xm,indexing='ij')\n",
        "yi_,ym_ = np.meshgrid(yi,ym,indexing='ij')\n",
        "\n",
        "dx = xi_ - xm_\n",
        "dy = yi_ - ym_\n",
        "\n",
        "# Note that in python * indicates elementwise multiplication, not matrix multiplication. \n",
        "Rdi = np.sqrt(dx*dx + dy*dy)\n",
        "\n",
        "# Build the data-data covariance\n",
        "Cdi = np.exp(-Rdi**2/b**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJgVkfQw9Weu"
      },
      "source": [
        "# interpolated observed data\n",
        "Di = np.mean(vm) + np.matmul(Cdi,np.matmul(np.linalg.inv(Cdd),(np.array(vm)-np.mean(vm))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-o3pa_B-bQb"
      },
      "source": [
        "fig = plt.figure(figsize=(16,6))\n",
        "\n",
        "ax  = fig.add_subplot(1,2,1)\n",
        "cax = ax.contourf(lon_, lat_, temp[:,:,k],32,cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Temperature at depth ' + str(z[0,k]) + 'm')\n",
        "cbar = fig.colorbar(cax)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "cax = ax.scatter(loni, lati, c=Di, s=20, edgecolors=\"none\",cmap=plt.cm.coolwarm,vmax=24,vmin=2.4)\n",
        "ax.set(title='Optimally interpolated data (deg C)')\n",
        "cbar = fig.colorbar(cax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgyhGOz_dBKF"
      },
      "source": [
        "------------------\n",
        "**Exercises**\n",
        "\n",
        "- You will notice that the optimally interpolated temperature is much smoother than the true temperature field. Why is this? How might you \"sharpen\" the interpolated field? \n",
        "\n",
        "- Your plot of the optimally interpolated data will have gaps in it because it does not include the gridpoints where you made measurements. Fill in these gaps by adding a extra line above to plot the measured values of temperature superimposed upon the optimally interpolated values: \n",
        "```\n",
        "cax = ax.scatter(lonm, latm, c=vm, s=20, edgecolors=\"none\")\n",
        "```\n",
        "How does this change the plot of temperature? What if you were to use the \"reconstructed\" data instead of the actual measured values? \n",
        "\n",
        "- We assumed that the data set we used was *unbiased* in the sense that the mean of the data should be close to the mean of the true temperature field. How accurate is this? What happens if you significantly reduce the number of data points? \n",
        "\n",
        "- The values of $a$ and $b$ were derived from the data itself. But there may be biases in these values due to undersampling, noise, etc. Try different values for $a$ and $b$ and see what this does to the covariance function and the reconstructed data. \n",
        "\n",
        "- Instead of the Gaussian function, we could have used a so-called Markov function: \n",
        "$$\n",
        "C_M(r) = a \\left( 1 + \\frac{r}{b} \\right) \\exp \\left( - r / b \\right). \n",
        "$$\n",
        "Modify the code above to fit the data to both a Gaussian and a Markov function and determine which produces a better fit. \n",
        "\n",
        "------------------\n",
        "\n",
        "**Challenge exercises:**\n",
        "\n",
        "- Did we need to calculate the full inverse of `Cdd` in our calculation of `D0`? Is there a more computationally efficient approach? \n",
        "\n",
        "- Research other techniques for fitting a function to data and apply them to our example. \n",
        "\n",
        "------------------\n"
      ]
    }
  ]
}